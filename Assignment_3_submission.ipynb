{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkaOHvO+F9gjLrHBRVbcWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madeleine-karlsson/AI-Course-Submission/blob/main/Assignment_3_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revealing representative \"typical\" day-types using traffic data observation and clustering\n"
      ],
      "metadata": {
        "id": "bNZ8f4ZU9L9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "5u4FKVZ69Zmo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCabqZk39GFH"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
        "import sklearn.metrics.pairwise as dis_lib\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from matplotlib.patches import Polygon\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib import gridspec\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib import colors\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#upload the dataset by downloading both datasets from canvas and upload it on colab\n",
        "\n",
        "data_df = pd.read_csv(\"dataset_exercise_5_clustering_highway_traffic.csv\",sep=\";\")\n",
        "data_df"
      ],
      "metadata": {
        "id": "q8Og3SOaWsai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the DataFrame 'data_df' by columns \"Date\" and \"Interval_5\"\n",
        "data_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted DataFrame\n",
        "days = np.unique(data_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days\n",
        "ndays = len(days)\n",
        "\n",
        "# Group the DataFrame 'data_df' by the \"Date\" column\n",
        "day_subsets_df = data_df.groupby([\"Date\"])\n",
        "\n",
        "# Define the total number of 5-minute intervals in a day\n",
        "nintvals = 288\n",
        "\n",
        "# Create a matrix 'vectorized_day_dataset' filled with NaN values\n",
        "vectorized_day_dataset = np.zeros((ndays, nintvals))\n",
        "vectorized_day_dataset.fill(np.nan)\n",
        "\n",
        "# Loop through each unique day\n",
        "for i in range(0, ndays):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame\n",
        "        df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset'\n",
        "        vectorized_day_dataset[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset'\n",
        "print(vectorized_day_dataset)"
      ],
      "metadata": {
        "id": "m7jI-fa7XE9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Data exploration"
      ],
      "metadata": {
        "id": "0scbwaPEYBMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('number of nans',np.sum(np.isnan(vectorized_day_dataset)))\n",
        "print('rate of nans',np.sum(np.isnan(vectorized_day_dataset))/(ndays*nintvals))"
      ],
      "metadata": {
        "id": "Z3A35bUxX0Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nans_per_time = np.sum(np.isnan(vectorized_day_dataset),0)\n",
        "print(nans_per_time.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# Create an array 'x_axis' representing the 5-minute intervals\n",
        "x_axis = np.arange(0, nintvals, 1, dtype=int)\n",
        "# Initialize an empty list 'x_axis_hours' to store time values in hours\n",
        "x_axis_hours = []\n",
        "# Convert interval indices to hours and append them to 'x_axis_hours'\n",
        "for i in range(0, len(x_axis)):\n",
        "  x_axis_hours.append(float(x_axis[i]*5)/60)\n",
        "ax.bar(x_axis_hours,height=nans_per_time)\n",
        "\n",
        "\n",
        "ax.set_ylabel('number of missing values')\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "ax.set_title('Time profile of missing values')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LSVLHmIkYMU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of days with missing values\n",
        "nans_per_day = np.sum(np.isnan(vectorized_day_dataset),1)\n",
        "print('number of days with missing value',np.size(np.where(nans_per_day > 0),1))"
      ],
      "metadata": {
        "id": "2No9r0qdYS86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()# a convenient way to create a new figure and a set of subplots.\n",
        "ax.plot(np.array([x_axis_hours,]*ndays).transpose(),vectorized_day_dataset.transpose(),color='#444444',alpha=0.05)\n",
        "# Above line plots the dataset with specified color and transparency.\n",
        "ax.plot(x_axis_hours,np.transpose(np.nanmean(vectorized_day_dataset,0)),color='black')\n",
        "# Above line plots the average of the dataset in black color.\n",
        "\n",
        "ax.set_ylabel('Average flow')\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "plt.xlim(0,24)\n",
        "ax.set_title('Daily profile of flow dynamic')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "waT6JOgjZIcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()  # This line is a convenient way to create a new figure and a set of subplots.\n",
        "\n",
        "# Create a boxplot for the dataset\n",
        "boxplot = ax.boxplot(vectorized_day_dataset.T, patch_artist=True)\n",
        "\n",
        "# Customize the boxplot appearance\n",
        "for patch in boxplot['boxes']:\n",
        "    patch.set_facecolor('#444444')  # Set the box color to gray\n",
        "for median in boxplot['medians']:\n",
        "    median.set(color='black', linewidth=2)  # Set median line color to black\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Flow')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "\n",
        "# Set the x-axis limits to be between 0 and 24\n",
        "plt.xlim(0, 24)\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Daily Profile of Flow Dynamics (Boxplot)')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BwdGk0POZo2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an array 'day_of_week' to store the day of the week for each unique date\n",
        "day_of_week = np.zeros((ndays))\n",
        "\n",
        "# Loop through each unique date\n",
        "for i in range(0, ndays):\n",
        "    # Parse the current date from a string to a datetime object\n",
        "    day_dt = datetime.datetime.strptime(str(days[i]), '%Y%m%d')\n",
        "\n",
        "    # Get the day of the week (1 for Monday, 2 for Tuesday, ..., 7 for Sunday)\n",
        "    day_of_week[i] = day_dt.isoweekday()"
      ],
      "metadata": {
        "id": "NerPXbRwZyZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Iterate through each day of the week (from 1 to 7)\n",
        "for i in range(1, 8):\n",
        "    # Find the indices of days that correspond to the current day of the week\n",
        "    day_of_week_index_t = np.where(day_of_week == i)\n",
        "\n",
        "    # Calculate the number of days that match the current day of the week\n",
        "    ndays_t = np.size(day_of_week_index_t[0])\n",
        "\n",
        "    # Plot the average flow for the current day of the week\n",
        "    ax.plot(x_axis_hours,\n",
        "            np.nanmean(vectorized_day_dataset[day_of_week_index_t[0], :].transpose(), 1),\n",
        "            label='day-of-week ' + str(i))\n",
        "    # This line plots the average flow for the current day of the week.\n",
        "    # 'np.nanmean()' calculates the mean while handling NaN values.\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Average flow')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "\n",
        "# Set the x-axis limits to be between 0 and 24\n",
        "plt.xlim(0, 24)\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Daily Profile of Flow Dynamics')\n",
        "\n",
        "# Add a legend indicating the day of the week\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VaIKgc3gZ0Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Clustering"
      ],
      "metadata": {
        "id": "29WVQYPWWM1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset without missing values\n",
        "vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "# Make clusters\n",
        "n_clusters_range = [3, 5, 7, 9, 12, 15]\n",
        "eps_range = [500]\n",
        "min_samples_range = [2, 3, 5]\n",
        "clusters = []\n",
        "\n",
        "for n_clusters in n_clusters_range:\n",
        "  kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans)\n",
        "  clusters.append(['KMeans', n_clusters, None, None, kmeans.labels_, None, None, None, None, None])\n",
        "\n",
        "for n_clusters in n_clusters_range:\n",
        "  agglomerative = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans)\n",
        "  clusters.append(['Agglomerative', n_clusters, None, None, agglomerative.labels_, None, None, None, None, None])\n",
        "\n",
        "for eps in eps_range:\n",
        "  for min_samples in min_samples_range:\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(vectorized_day_dataset_no_nans)\n",
        "    clusters.append(['DBSCAN', None, eps, min_samples, dbscan.labels_, None, None, None, None, None])\n",
        "\n",
        "for n_clusters in n_clusters_range:\n",
        "  gmm = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans)\n",
        "  clusters.append(['GMM', n_clusters, None, None, gmm, None, None, None, None, None])"
      ],
      "metadata": {
        "id": "RE3H9jNua4vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Clustering evaluation"
      ],
      "metadata": {
        "id": "upBwZiktcK5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# External evaluation dataset\n",
        "\n",
        "# Read the evaluation dataset from a CSV file\n",
        "data_eval_df = pd.read_csv(\"evaluation_dataset_exercise_5_clustering_highway_traffic.csv\", sep=\";\")\n",
        "\n",
        "# Sort the evaluation DataFrame by columns \"Date\" and \"Interval_5\"\n",
        "data_eval_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted evaluation DataFrame\n",
        "days_eval = np.unique(data_eval_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days in the evaluation dataset\n",
        "ndays_eval = len(days_eval)\n",
        "\n",
        "# Group the evaluation DataFrame by the \"Date\" column\n",
        "day_eval_subsets_df = data_eval_df.groupby([\"Date\"])\n",
        "\n",
        "# Initialize a matrix 'vectorized_day_dataset_eval' filled with NaN values\n",
        "vectorized_day_dataset_eval = np.zeros((ndays_eval, nintvals))\n",
        "vectorized_day_dataset_eval.fill(np.nan)\n",
        "# This section initializes a 2D array to store the evaluation dataset and fills it with NaN values.\n",
        "\n",
        "# Loop through each unique day in the evaluation dataset\n",
        "for i in range(0, ndays_eval):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame (this line is redundant)\n",
        "        df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset_eval'\n",
        "        vectorized_day_dataset_eval[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset_eval'\n",
        "print(vectorized_day_dataset_eval)\n"
      ],
      "metadata": {
        "id": "pIIXw8NBXHvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of NaN values in the evaluation dataset\n",
        "print('Number of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)))\n",
        "\n",
        "# Calculate the rate of NaN values in the evaluation dataset\n",
        "print('Rate of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)) / (ndays_eval * nintvals))\n",
        "\n",
        "# Calculate the number of days with missing values\n",
        "nans_per_day_eval = np.sum(np.isnan(vectorized_day_dataset_eval), 1)\n",
        "print('Number of days with missing values:', np.size(np.where(nans_per_day_eval > 0)))\n",
        "\n",
        "# Filter out days with no missing values and create a new dataset\n",
        "vectorized_day_dataset_no_nans_eval = vectorized_day_dataset_eval[np.where(nans_per_day_eval == 0)[0], :]\n",
        "days_not_nans_eval = days_eval[np.where(nans_per_day_eval == 0)[0]]\n",
        "\n",
        "# Calculate the final number of days in the evaluation dataset after removing missing values\n",
        "print('Final number of days in evaluation dataset:', len(days_not_nans_eval))\n",
        "\n",
        "# Print the list of days in the evaluation dataset with no missing values\n",
        "print('List of days without missing values:', days_not_nans_eval)\n",
        "\n",
        "# Calculate the total number of days in the filtered evaluation dataset\n",
        "ndays_eval_not_nans = len(days_not_nans_eval)"
      ],
      "metadata": {
        "id": "C-sgG8AQd1lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to find the closest centroid to a new data point within a specified day-time interval range\n",
        "def find_the_closest_centroid(centroids, new_day, from_interval: int, to_interval: int):\n",
        "    closest_centroid = None\n",
        "    closest_dist = None\n",
        "\n",
        "    # Iterate through each centroid\n",
        "    for i in range(0, len(centroids)):\n",
        "        # Calculate the Euclidean distance between the centroid and the new data point\n",
        "        ed_t = dis_lib.paired_distances(centroids[i], new_day, metric='euclidean')\n",
        "\n",
        "        # Check if the current centroid is closer than the previously closest one\n",
        "        if closest_centroid is None or closest_dist > ed_t:\n",
        "            closest_centroid = i\n",
        "            closest_dist = ed_t\n",
        "\n",
        "    return closest_centroid"
      ],
      "metadata": {
        "id": "lJDkS0TDeFZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalute the cluster methods\n",
        "for i in range(len(clusters)):\n",
        "  cluster_labels = clusters[i][4]\n",
        "\n",
        "  # Internal evaluation\n",
        "  clusters[i][5] = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  clusters[i][6] = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  clusters[i][7] = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "\n",
        "  # External evalutation (evaluating short-term prediction performance)\n",
        "\n",
        "  # Initialize a list to store centroid data\n",
        "  centroids = []\n",
        "\n",
        "  # Calculate centroids for each cluster\n",
        "  for j in np.unique(cluster_labels):\n",
        "    centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == j)[0], :], 0).reshape(1, nintvals)\n",
        "    centroids.append(centroid)\n",
        "\n",
        "  # Define the number of past intervals to consider for classification\n",
        "  n_past_intervals_for_classification = 5\n",
        "\n",
        "  # Initialize variables to calculate accuracy metrics\n",
        "  total_mae = 0\n",
        "  total_mape = 0\n",
        "  prediction_counts = 0\n",
        "\n",
        "\n",
        "  # Loop through each day in the evaluation dataset with no missing values\n",
        "  for k in range(0, ndays_eval_not_nans):\n",
        "    # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
        "    for l in range(n_past_intervals_for_classification, nintvals - 1):\n",
        "        # Find the closest centroid for the current data point\n",
        "        centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
        "\n",
        "        # Predict the value for the next interval\n",
        "        predicted_value = centroids[centroid_index][0, j + 1]\n",
        "\n",
        "        # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
        "        mae_t  = abs(predicted_value - vectorized_day_dataset_no_nans_eval[k][l + 1])\n",
        "        mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[k][l + 1]) / float(vectorized_day_dataset_no_nans_eval[k][l + 1])\n",
        "\n",
        "        # Accumulate MAE, MAPE, and count of predictions\n",
        "        total_mae += mae_t\n",
        "        total_mape += mape_t\n",
        "        prediction_counts += 1\n",
        "\n",
        "  clusters[i][8] = total_mae/prediction_counts\n",
        "  clusters[i][9] = total_mape/prediction_counts\n",
        "\n"
      ],
      "metadata": {
        "id": "-BFcd-ixojfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame(clusters, columns=['Method', 'n_clusters', 'eps', 'min_samples', 'cluster_labels', 'Silhouette score', 'Davies-Bouldin score', 'Calinski-Harabasz score', 'MAE', 'MAPE'])\n",
        "result_df"
      ],
      "metadata": {
        "id": "MW9kklqhoQAK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}